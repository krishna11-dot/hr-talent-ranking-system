{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GIsd8CIAlAtC",
        "outputId": "fe7e7121-f0e7-47a4-f2a2-36b970ec30f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload your Excel file...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8fa31f08-0602-4756-9bb8-98c46210dadd\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8fa31f08-0602-4756-9bb8-98c46210dadd\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving potential-talents.xlsx to potential-talents (3).xlsx\n",
            "\n",
            "Loading data from potential-talents (3).xlsx...\n",
            "\n",
            "Input DataFrame Structure:\n",
            "------------------------------\n",
            "\n",
            "Columns: ['id', 'job_title', 'location', 'connection', 'fit']\n",
            "\n",
            "Shape: (104, 5)\n",
            "\n",
            "Sample of input data:\n",
            "   id                                          job_title  \\\n",
            "0   1  2019 C.T. Bauer College of Business Graduate (...   \n",
            "1   2  Native English Teacher at EPIK (English Progra...   \n",
            "2   3              Aspiring Human Resources Professional   \n",
            "3   4             People Development Coordinator at Ryan   \n",
            "4   5    Advisory Board Member at Celal Bayar University   \n",
            "\n",
            "                              location connection  fit  \n",
            "0                       Houston, Texas         85  NaN  \n",
            "1                               Kanada      500+   NaN  \n",
            "2  Raleigh-Durham, North Carolina Area         44  NaN  \n",
            "3                        Denton, Texas      500+   NaN  \n",
            "4                       İzmir, Türkiye      500+   NaN  \n",
            "Starting data processing...\n",
            "\n",
            "=== Data Processing Steps ===\n",
            "--------------------------------------------------\n",
            "\n",
            "Location and Connection Transformations:\n",
            "                     Original Location               Standardized Location Original Connection  Normalized Connection\n",
            "0                       Houston, Texas                texas, united states                  85                    0.9\n",
            "2  Raleigh-Durham, North Carolina Area  north carolina area, united states                  44                    0.8\n",
            "5           Greater New York City Area             new york, united states                   1                    0.8\n",
            "\n",
            "Connection Normalization Rules:\n",
            "- Connections ≤ 50: Score = 0.8\n",
            "- Connections 51-200: Score = 0.9\n",
            "- Connections > 200: Score = 1.0\n",
            "\n",
            "Data Quality Report:\n",
            "==================================================\n",
            "\n",
            "Total records: 71\n",
            "\n",
            "Unique job titles: 8\n",
            "\n",
            "Most common job titles:\n",
            "processed_job_title\n",
            "aspiring human resources       35\n",
            "seeking human resources        10\n",
            "human resources                10\n",
            "human resources specialist      6\n",
            "human resources coordinator     4\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Unique locations: 19\n",
            "\n",
            "Location distribution:\n",
            "processed_location\n",
            "texas, united states                  17\n",
            "california, united states              9\n",
            "north carolina area, united states     8\n",
            "new york, united states                7\n",
            "canada                                 7\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Score Distribution:\n",
            "\n",
            "Title Similarity Scores:\n",
            "Mean: 0.817\n",
            "Median: 1.000\n",
            "Max: 1.000\n",
            "\n",
            "Top 10 Candidates:\n",
            "    id       processed_job_title  title_similarity  normalized_connections  \\\n",
            "52  53   seeking human resources               1.0                     1.0   \n",
            "39  40   seeking human resources               1.0                     1.0   \n",
            "29  30   seeking human resources               1.0                     1.0   \n",
            "61  62   seeking human resources               1.0                     1.0   \n",
            "9   10   seeking human resources               1.0                     1.0   \n",
            "27  28   seeking human resources               1.0                     1.0   \n",
            "74  75   seeking human resources               1.0                     1.0   \n",
            "93  94   seeking human resources               1.0                     1.0   \n",
            "81  82  aspiring human resources               1.0                     0.9   \n",
            "56  57  aspiring human resources               1.0                     0.9   \n",
            "\n",
            "    final_score  rank  \n",
            "52     1.003758   1.0  \n",
            "39     1.003758   1.0  \n",
            "29     1.003758   1.0  \n",
            "61     1.003758   1.0  \n",
            "9      1.003758   1.0  \n",
            "27     1.003758   1.0  \n",
            "74     1.003758   1.0  \n",
            "93     1.003758   1.0  \n",
            "81     0.992386   2.0  \n",
            "56     0.992386   2.0  \n",
            "\n",
            "Starring candidate 75 (7th in initial ranking)...\n",
            "\n",
            "Before starring - Candidate 75:\n",
            "id                                          75\n",
            "processed_job_title    seeking human resources\n",
            "final_score                           1.003758\n",
            "rank                                       1.0\n",
            "Name: 74, dtype: object\n",
            "\n",
            "After starring - Candidate 75:\n",
            "id                                          75\n",
            "processed_job_title    seeking human resources\n",
            "final_score                           2.250926\n",
            "rank                                       1.0\n",
            "Name: 74, dtype: object\n",
            "\n",
            "Top candidates after starring:\n",
            "    id      processed_job_title           processed_location  final_score  \\\n",
            "74  75  seeking human resources    california, united states     2.250926   \n",
            "9   10  seeking human resources    greater philadelphia area     1.711407   \n",
            "27  28  seeking human resources      illinois, united states     1.711407   \n",
            "29  30  seeking human resources      illinois, united states     1.711407   \n",
            "39  40  seeking human resources    greater philadelphia area     1.711407   \n",
            "52  53  seeking human resources    greater philadelphia area     1.711407   \n",
            "61  62  seeking human resources    greater philadelphia area     1.711407   \n",
            "93  94  seeking human resources  amerika birleşik devletleri     1.711407   \n",
            "69  70  seeking human resources      virginia, united states     1.688318   \n",
            "98  99  seeking human resources   nevada area, united states     1.644321   \n",
            "\n",
            "    rank  \n",
            "74   1.0  \n",
            "9    2.0  \n",
            "27   2.0  \n",
            "29   2.0  \n",
            "39   2.0  \n",
            "52   2.0  \n",
            "61   2.0  \n",
            "93   2.0  \n",
            "69   3.0  \n",
            "98   4.0  \n",
            "\n",
            "Candidates most similar to starred candidate:\n",
            "    id      processed_job_title           processed_location  final_score  \\\n",
            "74  75  seeking human resources    california, united states     2.250926   \n",
            "29  30  seeking human resources      illinois, united states     1.711407   \n",
            "27  28  seeking human resources      illinois, united states     1.711407   \n",
            "93  94  seeking human resources  amerika birleşik devletleri     1.711407   \n",
            "9   10  seeking human resources    greater philadelphia area     1.711407   \n",
            "\n",
            "    rank  \n",
            "74   1.0  \n",
            "29   2.0  \n",
            "27   2.0  \n",
            "93   2.0  \n",
            "9    2.0  \n",
            "\n",
            "Processing complete!\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "\n",
        "A comprehensive system for processing and analyzing HR candidate data, with focus on\n",
        "identifying potential HR talent through natural language processing and machine learning.\n",
        "\n",
        "Example usage:\n",
        "    # Initialize processor\n",
        "    processor = HRDataProcessor()\n",
        "\n",
        "    # Load data (in Google Colab)\n",
        "    from google.colab import files\n",
        "    uploaded = files.upload()\n",
        "    filename = list(uploaded.keys())[0]\n",
        "    df = pd.read_excel(filename)\n",
        "\n",
        "    # Process data\n",
        "    results = processor.process_data(df)\n",
        "\"\"\"\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "class HRDataProcessor:\n",
        "    \"\"\"\n",
        "    A class for processing HR candidate data and ranking potential talent.\n",
        "\n",
        "    This class implements various NLP and ML techniques to:\n",
        "    1. Clean and standardize job titles and locations\n",
        "    2. Process connection data\n",
        "    3. Calculate similarity scores\n",
        "    4. Implement unsupervised ranking\n",
        "    5. Handle starred candidates\n",
        "\n",
        "    Example:\n",
        "        processor = HRDataProcessor()\n",
        "        df = pd.read_excel('candidates.xlsx')\n",
        "        results = processor.process_data(df)\n",
        "        processor.star_candidate(results.iloc[0]['id'])\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the HRDataProcessor with necessary NLP tools and mapping dictionaries.\n",
        "\n",
        "        Sets up:\n",
        "        - NLP tools (lemmatizer, stop words, vectorizer)\n",
        "        - Target phrases for HR roles\n",
        "        - HR-related term mappings\n",
        "        - Standard job titles\n",
        "        - Terms and patterns to remove\n",
        "        - Location mappings\n",
        "        \"\"\"\n",
        "        # Initialize NLP tools\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.vectorizer = TfidfVectorizer()\n",
        "        self.starred_candidates = set()\n",
        "        self.df = None\n",
        "\n",
        "        # Target phrases (high-priority candidates)\n",
        "        self.target_phrases = [\n",
        "            'aspiring human resources',\n",
        "            'seeking human resources'\n",
        "        ]\n",
        "\n",
        "        # Core HR-related terms with standardized mappings\n",
        "        self.hr_mappings = {\n",
        "            'hr': 'human resources',\n",
        "            'chro': 'chief human resources officer',\n",
        "            'hro': 'human resources officer',\n",
        "            'hrbp': 'human resources business partner',\n",
        "            'people and culture': 'human resources',\n",
        "            'people operations': 'human resources',\n",
        "            'talent acquisition': 'human resources',\n",
        "            'recruitment': 'human resources',\n",
        "            'personnel': 'human resources'\n",
        "        }\n",
        "\n",
        "        # Standard HR job titles for consistency\n",
        "        self.hr_job_titles = {\n",
        "            'generalist': 'human resources generalist',\n",
        "            'coordinator': 'human resources coordinator',\n",
        "            'specialist': 'human resources specialist',\n",
        "            'manager': 'human resources manager',\n",
        "            'director': 'human resources director',\n",
        "            'assistant': 'human resources assistant',\n",
        "            'analyst': 'human resources analyst'\n",
        "        }\n",
        "\n",
        "        # Terms to remove (typically noise or irrelevant terms)\n",
        "        self.remove_terms = {\n",
        "            'current', 'student', 'graduate', 'member',\n",
        "            'learning', 'development', 'teacher', 'instructor',\n",
        "            'trainer', 'professor', 'educator'\n",
        "        }\n",
        "\n",
        "        # Patterns to remove (regex patterns for cleaning)\n",
        "        self.patterns_to_remove = [\n",
        "            r'\\b[A-Z]\\.[A-Z]\\.',  # Remove initials\n",
        "            r'\\b\\d{4}\\b',         # Remove years\n",
        "            r'\\b\\w+\\s+(university|college|institute|school|academy)\\b',\n",
        "            r'\\buniversity\\b',\n",
        "            r'\\bcollege\\b',\n",
        "            r'\\binstitute\\b',\n",
        "            r'\\bschool\\b',\n",
        "            r'\\blearning and development\\b'\n",
        "        ]\n",
        "\n",
        "        # Location standardization mappings\n",
        "        self.location_mappings = {\n",
        "            'houston': 'texas, united states',\n",
        "            'dallas': 'texas, united states',\n",
        "            'austin': 'texas, united states',\n",
        "            'san francisco': 'california, united states',\n",
        "            'los angeles': 'california, united states',\n",
        "            'bay area': 'california, united states',\n",
        "            'kanada': 'canada',\n",
        "            'new york city': 'new york, united states',\n",
        "            'greater new york city area': 'new york, united states'\n",
        "        }\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"\n",
        "        Preprocess text data with enhanced lemmatization and cleaning.\n",
        "\n",
        "        Args:\n",
        "            text (str): Input text to process\n",
        "\n",
        "        Returns:\n",
        "            str: Cleaned and preprocessed text\n",
        "\n",
        "        Example:\n",
        "            >>> processor = HRDataProcessor()\n",
        "            >>> processor.preprocess_text(\"HR Manager at ABC University\")\n",
        "            'human resources manager'\n",
        "        \"\"\"\n",
        "        if not isinstance(text, str):\n",
        "            return \"\"\n",
        "\n",
        "        # Basic cleaning\n",
        "        text = text.lower().strip()\n",
        "        text = unicodedata.normalize('NFKD', text).encode('ASCII', 'ignore').decode('utf-8')\n",
        "\n",
        "        # Remove patterns\n",
        "        for pattern in self.patterns_to_remove:\n",
        "            text = re.sub(pattern, ' ', text, flags=re.IGNORECASE)\n",
        "\n",
        "        # Remove special characters and extra spaces\n",
        "        text = re.sub(r'[^a-z\\s]', ' ', text)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        # Tokenize and lemmatize with handling of bigrams\n",
        "        tokens = word_tokenize(text)\n",
        "        lemmatized_tokens = []\n",
        "\n",
        "        i = 0\n",
        "        while i < len(tokens):\n",
        "            if i < len(tokens) - 1:\n",
        "                bigram = f\"{tokens[i]} {tokens[i+1]}\"\n",
        "                if bigram in self.hr_mappings:\n",
        "                    lemmatized_tokens.extend(self.hr_mappings[bigram].split())\n",
        "                    i += 2\n",
        "                    continue\n",
        "\n",
        "            token = tokens[i]\n",
        "            if token in self.hr_mappings:\n",
        "                lemmatized_tokens.extend(self.hr_mappings[token].split())\n",
        "            elif token not in self.stop_words and token not in self.remove_terms:\n",
        "                lemmatized_tokens.append(self.lemmatizer.lemmatize(token))\n",
        "            i += 1\n",
        "\n",
        "        return ' '.join(lemmatized_tokens)\n",
        "\n",
        "    def clean_job_title(self, title):\n",
        "        \"\"\"\n",
        "        Clean and standardize job titles with focus on HR roles.\n",
        "\n",
        "        Args:\n",
        "            title (str): Input job title\n",
        "\n",
        "        Returns:\n",
        "            str: Standardized job title\n",
        "\n",
        "        Example:\n",
        "            >>> processor = HRDataProcessor()\n",
        "            >>> processor.clean_job_title(\"Aspiring HR Professional\")\n",
        "            'aspiring human resources'\n",
        "        \"\"\"\n",
        "        if not isinstance(title, str):\n",
        "            return \"\"\n",
        "\n",
        "        processed_title = self.preprocess_text(title)\n",
        "\n",
        "        # First priority: Aspiring/seeking HR roles\n",
        "        if 'aspiring' in processed_title and ('hr' in processed_title or 'human resource' in processed_title):\n",
        "            return 'aspiring human resources'\n",
        "\n",
        "        if 'seeking' in processed_title and ('hr' in processed_title or 'human resource' in processed_title):\n",
        "            return 'seeking human resources'\n",
        "\n",
        "        # Second priority: Standard HR roles\n",
        "        if any(term in processed_title for term in ['hr', 'human resource']):\n",
        "            for role, full_title in self.hr_job_titles.items():\n",
        "                if role in processed_title:\n",
        "                    return full_title\n",
        "            return 'human resources'\n",
        "\n",
        "        return ''\n",
        "\n",
        "    def standardize_location(self, location):\n",
        "        \"\"\"\n",
        "        Standardize location names for consistency.\n",
        "\n",
        "        Args:\n",
        "            location (str): Input location string\n",
        "\n",
        "        Returns:\n",
        "            str: Standardized location string\n",
        "\n",
        "        Example:\n",
        "            >>> processor = HRDataProcessor()\n",
        "            >>> processor.standardize_location(\"NYC\")\n",
        "            'new york, united states'\n",
        "        \"\"\"\n",
        "        if not isinstance(location, str):\n",
        "            return \"unknown\"\n",
        "\n",
        "        location = location.lower().strip()\n",
        "\n",
        "        # Handle specific mappings\n",
        "        for key, value in self.location_mappings.items():\n",
        "            if key in location:\n",
        "                return value\n",
        "\n",
        "        # General location processing\n",
        "        if ',' in location:\n",
        "            city, region = location.split(',', 1)\n",
        "            region = region.strip()\n",
        "            if region.lower() not in ['canada', 'turkey', 'united states']:\n",
        "                return f\"{region.strip()}, united states\"\n",
        "\n",
        "        return location\n",
        "\n",
        "    def process_connections(self, connection):\n",
        "        \"\"\"\n",
        "        Process connection counts with gentle normalization.\n",
        "\n",
        "        Args:\n",
        "            connection (str/int): Connection count\n",
        "\n",
        "        Returns:\n",
        "            float: Normalized connection score\n",
        "\n",
        "        Example:\n",
        "            >>> processor = HRDataProcessor()\n",
        "            >>> processor.process_connections(\"500+\")\n",
        "            1.0\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if isinstance(connection, str):\n",
        "                value = int(connection.replace('+', '').strip())\n",
        "            else:\n",
        "                value = int(connection)\n",
        "\n",
        "            # Gentle normalization\n",
        "            if value <= 50:\n",
        "                return 0.8  # Base score for new profiles\n",
        "            elif value <= 200:\n",
        "                return 0.9\n",
        "            else:\n",
        "                return 1.0\n",
        "\n",
        "        except (ValueError, TypeError):\n",
        "            return 0.8\n",
        "\n",
        "    def calculate_similarity_scores(self, df):\n",
        "        \"\"\"\n",
        "        Calculate similarity scores based on job titles.\n",
        "\n",
        "        Args:\n",
        "            df (pandas.DataFrame): Input DataFrame\n",
        "\n",
        "        Returns:\n",
        "            numpy.array: Array of similarity scores\n",
        "        \"\"\"\n",
        "        processed_titles = df['processed_job_title'].fillna('')\n",
        "        scores = np.zeros(len(df))\n",
        "\n",
        "        # Exact matches get highest score\n",
        "        scores[processed_titles == 'aspiring human resources'] = 1.0\n",
        "        scores[processed_titles == 'seeking human resources'] = 1.0\n",
        "\n",
        "        # Partial matches get intermediate scores\n",
        "        for title in processed_titles.unique():\n",
        "            if 'human resources' in title and title not in self.target_phrases:\n",
        "                mask = processed_titles == title\n",
        "                scores[mask] = 0.5\n",
        "\n",
        "        return scores\n",
        "\n",
        "    def create_feature_matrix(self, df):\n",
        "        \"\"\"\n",
        "        Create feature matrix for clustering.\n",
        "\n",
        "        Args:\n",
        "            df (pandas.DataFrame): Input DataFrame\n",
        "\n",
        "        Returns:\n",
        "            numpy.array: Standardized feature matrix\n",
        "        \"\"\"\n",
        "        title_vectorizer = TfidfVectorizer(max_features=20)\n",
        "        title_features = title_vectorizer.fit_transform(\n",
        "            df['processed_job_title'].fillna('')\n",
        "        )\n",
        "\n",
        "        connection_features = df['normalized_connections'].values.reshape(-1, 1)\n",
        "\n",
        "        feature_matrix = np.hstack([\n",
        "            title_features.toarray(),\n",
        "            connection_features\n",
        "        ])\n",
        "\n",
        "        return StandardScaler().fit_transform(feature_matrix)\n",
        "\n",
        "    def implement_unsupervised_ranking(self, df):\n",
        "        \"\"\"\n",
        "        Implement unsupervised ranking using K-means clustering.\n",
        "\n",
        "        Args:\n",
        "            df (pandas.DataFrame): Input DataFrame\n",
        "\n",
        "        Returns:\n",
        "            numpy.array: Array of cluster-based scores\n",
        "        \"\"\"\n",
        "        if len(df) == 0:\n",
        "            return np.array([])\n",
        "\n",
        "        feature_matrix = self.create_feature_matrix(df)\n",
        "\n",
        "        n_clusters = min(5, len(df))\n",
        "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "\n",
        "        cluster_labels = kmeans.fit_predict(feature_matrix)\n",
        "        cluster_scores = np.zeros(len(df))\n",
        "\n",
        "        for cluster in range(n_clusters):\n",
        "            cluster_mask = cluster_labels == cluster\n",
        "            if np.any(cluster_mask):\n",
        "                cluster_scores[cluster_mask] = -np.linalg.norm(\n",
        "                    feature_matrix[cluster_mask] -\n",
        "                    kmeans.cluster_centers_[cluster],\n",
        "                    axis=1\n",
        "                )\n",
        "\n",
        "        cluster_scores = (cluster_scores - cluster_scores.min()) / (\n",
        "            cluster_scores.max() - cluster_scores.min()\n",
        "        )\n",
        "\n",
        "        return cluster_scores\n",
        "\n",
        "    def star_candidate(self, candidate_id):\n",
        "        \"\"\"\n",
        "        Star a candidate and boost similar profiles.\n",
        "\n",
        "        Args:\n",
        "            candidate_id (int): ID of candidate to star\n",
        "\n",
        "        Returns:\n",
        "            pandas.DataFrame: Updated DataFrame with new scores\n",
        "\n",
        "        Example:\n",
        "            >>> processor = HRDataProcessor()\n",
        "            >>> df = processor.process_data(input_df)\n",
        "            >>> processor.star_candidate(75)  # Stars candidate with ID 75\n",
        "        \"\"\"\n",
        "        if self.df is None:\n",
        "            print(\"Please process data first before starring candidates\")\n",
        "            return\n",
        "\n",
        "        if candidate_id not in self.df['id'].values:\n",
        "            print(f\"Candidate {candidate_id} not found\")\n",
        "            return\n",
        "\n",
        "        print(f\"\\nBefore starring - Candidate {candidate_id}:\")\n",
        "        before_data = self.df[self.df['id'] == candidate_id][\n",
        "            ['id', 'processed_job_title', 'final_score', 'rank']\n",
        "        ].iloc[0]\n",
        "        print(before_data)\n",
        "\n",
        "        # Get starred candidate's features\n",
        "        starred_candidate = self.df[self.df['id'] == candidate_id].iloc[0]\n",
        "\n",
        "        # Add to starred set\n",
        "        self.starred_candidates.add(candidate_id)\n",
        "\n",
        "        # Calculate title similarity with starred candidate\n",
        "        all_titles = list(self.df['processed_job_title']) + [starred_candidate['processed_job_title']]\n",
        "        title_vectors = self.vectorizer.fit_transform(all_titles)\n",
        "        similarities = cosine_similarity(\n",
        "            title_vectors[:-1],\n",
        "            title_vectors[-1:]\n",
        "        ).flatten()\n",
        "\n",
        "        # Calculate location boost\n",
        "        location_boost = (self.df['processed_location'] ==\n",
        "                         starred_candidate['processed_location']).astype(float) * 0.1\n",
        "\n",
        "        # Calculate connection similarity\n",
        "        connection_diff = abs(\n",
        "            self.df['normalized_connections'] -\n",
        "            starred_candidate['normalized_connections']\n",
        "        )\n",
        "        connection_boost = (1 - connection_diff) * 0.05\n",
        "\n",
        "        # Calculate total boost factor\n",
        "        boost_factor = 1 + (0.7 * similarities +\n",
        "                           0.2 * location_boost +\n",
        "                           0.1 * connection_boost)\n",
        "\n",
        "        # Apply boost to scores\n",
        "        self.df['final_score'] = self.df['final_score'] * boost_factor\n",
        "\n",
        "        # Extra boost for the starred candidate\n",
        "        self.df.loc[self.df['id'] == candidate_id, 'final_score'] *= 1.3\n",
        "\n",
        "        # Recalculate ranks\n",
        "        self.df['rank'] = self.df['final_score'].rank(method='dense', ascending=False)\n",
        "\n",
        "        self._display_starring_results(candidate_id, before_data, boost_factor)\n",
        "        return self.df\n",
        "\n",
        "    def _display_starring_results(self, candidate_id, before_data, boost_factor):\n",
        "        \"\"\"\n",
        "        Display results after starring a candidate (helper method).\n",
        "\n",
        "        Args:\n",
        "            candidate_id (int): ID of starred candidate\n",
        "            before_data (pandas.Series): Candidate data before starring\n",
        "            boost_factor (numpy.array): Boost factors for all candidates\n",
        "        \"\"\"\n",
        "        print(f\"\\nAfter starring - Candidate {candidate_id}:\")\n",
        "        after_data = self.df[self.df['id'] == candidate_id][\n",
        "            ['id', 'processed_job_title', 'final_score', 'rank']\n",
        "        ].iloc[0]\n",
        "        print(after_data)\n",
        "\n",
        "        # Show influenced candidates\n",
        "        print(\"\\nTop candidates after starring:\")\n",
        "        top_df = self.df.nsmallest(10, 'rank')[\n",
        "            ['id', 'processed_job_title', 'processed_location', 'final_score', 'rank']\n",
        "        ]\n",
        "        print(top_df)\n",
        "\n",
        "        # Show similar candidates that were boosted\n",
        "        boosted_mask = boost_factor > 1.1  # Show candidates with >10% boost\n",
        "        if boosted_mask.any():\n",
        "            print(\"\\nCandidates most similar to starred candidate:\")\n",
        "            similar_df = self.df[boosted_mask].sort_values('final_score', ascending=False)[\n",
        "                ['id', 'processed_job_title', 'processed_location', 'final_score', 'rank']\n",
        "            ].head()\n",
        "            print(similar_df)\n",
        "\n",
        "    def calculate_final_score(self, row):\n",
        "        \"\"\"\n",
        "        Calculate final score for a candidate with reduced connection impact.\n",
        "\n",
        "        Args:\n",
        "            row (pandas.Series): Row containing candidate data\n",
        "\n",
        "        Returns:\n",
        "            float: Final calculated score\n",
        "\n",
        "        Example:\n",
        "            >>> processor = HRDataProcessor()\n",
        "            >>> score = processor.calculate_final_score(candidate_row)\n",
        "        \"\"\"\n",
        "        base_score = (\n",
        "            0.85 * row['title_similarity'] +\n",
        "            0.15 * row['cluster_score']\n",
        "        )\n",
        "\n",
        "        # Only slightly modify base score with connections\n",
        "        connection_modifier = 1 + (0.05 * (row['normalized_connections'] - 0.8))\n",
        "        return base_score * connection_modifier\n",
        "\n",
        "    def process_data(self, df):\n",
        "        \"\"\"\n",
        "        Process input DataFrame through the complete pipeline.\n",
        "\n",
        "        Args:\n",
        "            df (pandas.DataFrame): Input DataFrame with candidate data\n",
        "\n",
        "        Returns:\n",
        "            pandas.DataFrame: Processed and ranked DataFrame\n",
        "\n",
        "        Example:\n",
        "            >>> processor = HRDataProcessor()\n",
        "            >>> df = pd.read_excel('candidates.xlsx')\n",
        "            >>> results = processor.process_data(df)\n",
        "        \"\"\"\n",
        "        print(\"\\nInput DataFrame Structure:\")\n",
        "        print(\"-\" * 30)\n",
        "        print(\"\\nColumns:\", df.columns.tolist())\n",
        "        print(\"\\nShape:\", df.shape)\n",
        "        print(\"\\nSample of input data:\")\n",
        "        print(df.head())\n",
        "\n",
        "        print(\"Starting data processing...\")\n",
        "\n",
        "        self.df = df.copy()\n",
        "\n",
        "        # Basic preprocessing\n",
        "        self.df['processed_job_title'] = df['job_title'].apply(self.clean_job_title)\n",
        "        self.df = self.df[self.df['processed_job_title'] != '']\n",
        "        self.df['processed_location'] = df['location'].apply(self.standardize_location)\n",
        "        self.df['normalized_connections'] = df['connection'].apply(self.process_connections)\n",
        "\n",
        "        # Calculate scores\n",
        "        self.df['title_similarity'] = self.calculate_similarity_scores(self.df)\n",
        "        self.df['cluster_score'] = self.implement_unsupervised_ranking(self.df)\n",
        "        self.df['final_score'] = self.df.apply(self.calculate_final_score, axis=1)\n",
        "\n",
        "        # Handle starred candidates\n",
        "        if self.starred_candidates:\n",
        "            self.df = self.star_candidate(list(self.starred_candidates)[0])\n",
        "\n",
        "        # Final ranking\n",
        "        self.df['rank'] = self.df['final_score'].rank(method='dense', ascending=False)\n",
        "\n",
        "        self.display_results(self.df)\n",
        "        return self.df.sort_values('rank')\n",
        "\n",
        "    def display_results(self, df):\n",
        "        \"\"\"\n",
        "        Display comprehensive results of the processing pipeline.\n",
        "\n",
        "        Args:\n",
        "            df (pandas.DataFrame): Processed DataFrame to display results for\n",
        "\n",
        "        Example:\n",
        "            >>> processor = HRDataProcessor()\n",
        "            >>> processor.display_results(results_df)\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        print(\"\\n=== Data Processing Steps ===\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "    # Format the transformations in a clear table\n",
        "        cleaning_example = pd.DataFrame({\n",
        "            'Original Location': df['location'].head(3),\n",
        "            'Standardized Location': df['processed_location'].head(3),\n",
        "            'Original Connection': df['connection'].head(3),\n",
        "            'Normalized Connection': df['normalized_connections'].head(3)\n",
        "        })\n",
        "\n",
        "        print(\"\\nLocation and Connection Transformations:\")\n",
        "        print(cleaning_example.to_string(index=True))\n",
        "\n",
        "        print(\"\\nConnection Normalization Rules:\")\n",
        "        print(\"- Connections ≤ 50: Score = 0.8\")\n",
        "        print(\"- Connections 51-200: Score = 0.9\")\n",
        "        print(\"- Connections > 200: Score = 1.0\")\n",
        "\n",
        "\n",
        "\n",
        "        print(\"\\nData Quality Report:\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        print(f\"\\nTotal records: {len(df)}\")\n",
        "        print(f\"\\nUnique job titles: {df['processed_job_title'].nunique()}\")\n",
        "        print(\"\\nMost common job titles:\")\n",
        "        print(df['processed_job_title'].value_counts().head())\n",
        "\n",
        "        print(f\"\\nUnique locations: {df['processed_location'].nunique()}\")\n",
        "        print(\"\\nLocation distribution:\")\n",
        "        print(df['processed_location'].value_counts().head())\n",
        "\n",
        "        print(\"\\nScore Distribution:\")\n",
        "        print(\"\\nTitle Similarity Scores:\")\n",
        "        print(f\"Mean: {df['title_similarity'].mean():.3f}\")\n",
        "        print(f\"Median: {df['title_similarity'].median():.3f}\")\n",
        "        print(f\"Max: {df['title_similarity'].max():.3f}\")\n",
        "\n",
        "        if self.starred_candidates:\n",
        "            print(\"\\nStarred Candidates:\")\n",
        "            starred_df = df[df['id'].isin(self.starred_candidates)]\n",
        "            print(starred_df[['id', 'processed_job_title', 'final_score', 'rank']])\n",
        "\n",
        "        print(\"\\nTop 10 Candidates:\")\n",
        "        display_cols = [\n",
        "            'id', 'processed_job_title', 'title_similarity',\n",
        "            'normalized_connections', 'final_score', 'rank'\n",
        "        ]\n",
        "        print(df.sort_values('rank')[display_cols].head(10))\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to run the HR Data Processor in Google Colab.\n",
        "\n",
        "    Example:\n",
        "        >>> processor, results = main()\n",
        "    \"\"\"\n",
        "    processor = HRDataProcessor()\n",
        "\n",
        "    try:\n",
        "        # Upload and load data\n",
        "        from google.colab import files\n",
        "        print(\"Please upload your Excel file...\")\n",
        "        uploaded = files.upload()\n",
        "\n",
        "        # Get the filename from uploaded files\n",
        "        filename = list(uploaded.keys())[0]\n",
        "\n",
        "        # Load and process data\n",
        "        print(f\"\\nLoading data from {filename}...\")\n",
        "        df = pd.read_excel(filename)\n",
        "\n",
        "        # Initial processing\n",
        "        results_df = processor.process_data(df)\n",
        "\n",
        "        # Example: Star a candidate (e.g., 7th ranked candidate)\n",
        "        seventh_candidate = results_df.iloc[6]['id']\n",
        "        print(f\"\\nStarring candidate {seventh_candidate} (7th in initial ranking)...\")\n",
        "        processor.star_candidate(seventh_candidate)\n",
        "\n",
        "        print(\"\\nProcessing complete!\")\n",
        "\n",
        "        return processor, results_df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in processing: {str(e)}\")\n",
        "        return None, None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    processor, results = main()"
      ]
    }
  ]
}